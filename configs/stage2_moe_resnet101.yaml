# Stage 2 MoE Fine-tuning - ResNet-101 Backbone
# High-capacity variant with 33 residual blocks (matches Strodthoff2020)
# Same hyperparameters as ResNet-18 baseline, only changing encoder depth
# Expected params: ~35M (encoder 32.2M + router + experts)

experiment: stage2_moe_resnet101
seed: 42

dataset:
  name: ptbxl
  path: data/processed/ptbxl/fs500/superclasses
  batch_size: 32  # Reduced from 64 due to larger model
  num_workers: 4
  patch_size: 64

model:
  patch_encoder_hidden: 128
  embedding_dim: 256
  num_experts: 3  # Rhythm, Morphology, Quality
  num_classes: 5  # NORM, MI, STTC, CD, HYP
  deep_encoder: true  # Use ResNet backbone
  encoder_depth: resnet101  # 33 residual blocks (~32.2M params)
  use_attention: true  # Attention Aggregation
  attention_heads: 4
  input_channels: 12  # 12-lead input

training:
  epochs: 50  # Shorter than Stage 1 since we're doing supervised finetuning
  lr: 0.0003  # Adjusted LR to reduce overfitting
  weight_decay: 0.002
  warmup_epochs: 5  # Warm-up for 5 epochs
  dropout: 0.3
  grad_clip: 1.0
  use_symbolic_gating: true
  lambda_symbolic: 0.7  # Weight for symbolic consistency loss (increased to enforce diversity)
  lambda_load_balance: 0.01  # Weight for load balancing loss (prevent expert collapse)
  use_focal_loss: false
  focal_gamma: 1.5

logging:
  checkpoint_dir: checkpoints/stage2_moe_resnet101
  log_interval: 50


